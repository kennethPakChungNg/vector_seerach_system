{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Text Preprocessing and Embedding Generation\n",
    "\n",
    "## Overview\n",
    "This notebook tests our text preprocessing pipeline and prepares for embedding generation using the DeepSeek model. We'll verify that our preprocessing handles both Portuguese and English text correctly before moving on to generating embeddings.\n",
    "\n",
    "## Understanding the Imports\n",
    "\n",
    "### System and Path Management\n",
    "```python\n",
    "import sys\n",
    "from pathlib import Path\n",
    "```\n",
    "- `sys`: Provides access to Python interpreter variables and functions\n",
    "- `pathlib.Path`: Modern way to handle file paths in Python, making it easier to work with directories and files\n",
    "\n",
    "### Project-Specific Imports\n",
    "```python\n",
    "from vectorshop.data.preprocessing import TextPreprocessor\n",
    "from vectorshop.config import DATA_FILES, RAW_DATA_DIR\n",
    "```\n",
    "These imports come from our own project structure:\n",
    "- `TextPreprocessor`: Our custom class for cleaning and standardizing text\n",
    "- `DATA_FILES` and `RAW_DATA_DIR`: Configuration settings for data file locations\n",
    "\n",
    "### Data Analysis Libraries\n",
    "```python\n",
    "import pandas as pd\n",
    "```\n",
    "- `pandas`: Library for data manipulation and analysis\n",
    "\n",
    "## Module Organization\n",
    "Our project is organized like a Python package:\n",
    "```\n",
    "vectorshop/\n",
    "├── vectorshop/\n",
    "│   ├── data/\n",
    "│   │   ├── preprocessing.py  # Contains TextPreprocessor\n",
    "│   ├── config.py            # Contains settings and paths\n",
    "```\n",
    "\n",
    "## Preprocessing Steps\n",
    "1. Text Cleaning\n",
    "   - Convert to lowercase\n",
    "   - Remove accents (important for Portuguese text)\n",
    "   - Remove special characters\n",
    "   - Remove extra whitespace\n",
    "\n",
    "2. Batch Processing\n",
    "   - Process multiple texts efficiently\n",
    "   - Maintain consistent cleaning across all products\n",
    "\n",
    "3. Combined Text Creation\n",
    "   - Merge product name, description, and category\n",
    "   - Create standardized format for embedding\n",
    "\n",
    "## Next Steps\n",
    "After verifying preprocessing works correctly, we'll:\n",
    "1. Set up the DeepSeek model\n",
    "2. Generate embeddings for sample products\n",
    "3. Test embedding quality\n",
    "4. Implement vector storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\User\\Desktop\\vectorshop\n",
      "Added c:\\Users\\User\\Desktop\\vectorshop to Python path\n",
      "Current working directory: c:\\Users\\User\\Desktop\\vectorshop\\notebooks\\notebooks\n",
      "Python path after update: c:\\Users\\User\\Desktop\\vectorshop\n",
      "Successfully imported TextPreprocessor\n",
      "Successfully imported config variables\n",
      "All imports completed\n",
      "\n",
      "Verifying directory structure:\n",
      "Does vectorshop directory exist? True\n",
      "Does preprocessing.py exist? True\n",
      "Does config.py exist? True\n"
     ]
    }
   ],
   "source": [
    "# First cell - Path setup and imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the correct project root (going up TWO levels from current notebook)\n",
    "# We're in notebooks/notebooks, so we need to go up twice\n",
    "project_root = Path.cwd().parent.parent  # Changed this line\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Add to Python path if not already there\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "# Let's add some debugging information\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "print(f\"Python path after update: {sys.path[0]}\")\n",
    "\n",
    "# Try importing our modules with better error handling\n",
    "try:\n",
    "    from vectorshop.data.preprocessing import TextPreprocessor\n",
    "    print(\"Successfully imported TextPreprocessor\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing TextPreprocessor: {e}\")\n",
    "\n",
    "try:\n",
    "    from vectorshop.config import DATA_FILES, RAW_DATA_DIR\n",
    "    print(\"Successfully imported config variables\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing config: {e}\")\n",
    "\n",
    "import pandas as pd\n",
    "print(\"All imports completed\")\n",
    "\n",
    "# Additional verification\n",
    "print(\"\\nVerifying directory structure:\")\n",
    "print(f\"Does vectorshop directory exist? {(project_root / 'vectorshop').exists()}\")\n",
    "print(f\"Does preprocessing.py exist? {(project_root / 'vectorshop' / 'data' / 'preprocessing.py').exists()}\")\n",
    "print(f\"Does config.py exist? {(project_root / 'vectorshop' / 'config.py').exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded olist_products_dataset.csv with 32951 rows\n",
      "Successfully loaded product_category_name_translation.csv with 71 rows\n",
      "\n",
      "Sample Product Data:\n",
      "==================================================\n",
      "\n",
      "Shape of sample data: (5, 10)\n",
      "\n",
      "Columns in dataset:\n",
      "- product_id\n",
      "- product_category_name\n",
      "- product_name_lenght\n",
      "- product_description_lenght\n",
      "- product_photos_qty\n",
      "- product_weight_g\n",
      "- product_length_cm\n",
      "- product_height_cm\n",
      "- product_width_cm\n",
      "- product_category_name_english\n",
      "\n",
      "First product details:\n",
      "------------------------------\n",
      "product_id: f819f0c84a64f02d3a5606ca95edd272\n",
      "product_category_name: relogios_presentes\n",
      "product_name_lenght: 59.0\n",
      "product_description_lenght: 452.0\n",
      "product_photos_qty: 1.0\n",
      "product_weight_g: 710.0\n",
      "product_length_cm: 19.0\n",
      "product_height_cm: 13.0\n",
      "product_width_cm: 14.0\n",
      "product_category_name_english: watches_gifts\n"
     ]
    }
   ],
   "source": [
    "# Second cell - Test data loading\n",
    "try:\n",
    "    # Import the loader\n",
    "    from vectorshop.data.load import OlistDataLoader\n",
    "    \n",
    "    # Create loader instance\n",
    "    loader = OlistDataLoader(RAW_DATA_DIR)\n",
    "    \n",
    "    # Get a sample of product data\n",
    "    sample_products = loader.get_sample_product_data(n_samples=5)  # Changed method name to match\n",
    "    \n",
    "    print(\"\\nSample Product Data:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nShape of sample data: {sample_products.shape}\")\n",
    "    print(f\"\\nColumns in dataset:\")\n",
    "    for col in sample_products.columns:\n",
    "        print(f\"- {col}\")\n",
    "    \n",
    "    print(\"\\nFirst product details:\")\n",
    "    print(\"-\"*30)\n",
    "    first_product = sample_products.iloc[0]\n",
    "    for col, value in first_product.items():\n",
    "        print(f\"{col}: {value}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded olist_products_dataset.csv with 32951 rows\n",
      "Successfully loaded product_category_name_translation.csv with 71 rows\n",
      "Successfully loaded olist_order_items_dataset.csv with 112650 rows\n",
      "Successfully loaded olist_order_reviews_dataset.csv with 99224 rows\n",
      "\n",
      "Complete Product Data Sample:\n",
      "==================================================\n",
      "\n",
      "Shape of full data: (3, 13)\n",
      "\n",
      "Columns in complete dataset:\n",
      "- product_id\n",
      "- product_category_name\n",
      "- product_name_lenght\n",
      "- product_description_lenght\n",
      "- product_photos_qty\n",
      "- product_weight_g\n",
      "- product_length_cm\n",
      "- product_height_cm\n",
      "- product_width_cm\n",
      "- product_category_name_english\n",
      "- review_comment_title\n",
      "- review_comment_message\n",
      "- review_score\n",
      "\n",
      "Detailed view of first product:\n",
      "------------------------------\n",
      "\n",
      "Basic Information:\n",
      "Product ID: f819f0c84a64f02d3a5606ca95edd272\n",
      "Category (PT): relogios_presentes\n",
      "Category (EN): watches_gifts\n",
      "\n",
      "Customer Reviews:\n",
      "Average Score: 4.09\n",
      "Review Comments:\n",
      "correta  | não usei o produto não mas parece bom | Eu comprei um relógio e me mandaram outro  | Melhor do Brasil | vou comprar de novo , gostei e é excelente os produtos obrigado! | na verdade o produto chegou um dia após a data prevista e não mim satisfez. mesmo assim agradeço\n",
      " | Muito bom chegou no prazo eu recomendo produto muito bom... | Superou minhas espectativas , muito bom produto . | Recebi em tempo hábil , produto superou minhas expectativas... satisfeito ...super recomendo !!\n",
      "UM ARRASO !!! | Muito bom ... a transportadora dentro do prazo ótimo. | Produto original | Recomendo, entregue muito antes do prazo predeterminado.  | Chegou muito antes do prazo recomendo | Ñ compriu seu prazo de entrega  | Produto excelente bom, bonito gostei muito atendeu as minhas expectativas, fiz o teste pra saber sobre prova dagua, tudo ok, eu recomendo nota mil.  | Nao recebi  | não recebi o produto  | Não recebi o produto até o momento... Segundo o site está entregue. Mas entregue à quem??? Eu não recebi.\n",
      "Pelo rastreio aparece PARADO nos correios do Rio de Janeiro.\n",
      "Aguardo retorno. | bom comprar com lannister | Só quê o ponteiro do segundo não para. Não sei porque geralmente ele para  | Ótimo relógio recomendo a todos ótimo qualidade \n"
     ]
    }
   ],
   "source": [
    "# Third cell - Test full product data loading\n",
    "try:\n",
    "    # Get a sample of complete product data\n",
    "    full_sample = loader.get_full_product_data(n_samples=3)\n",
    "    \n",
    "    print(\"\\nComplete Product Data Sample:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nShape of full data: {full_sample.shape}\")\n",
    "    print(f\"\\nColumns in complete dataset:\")\n",
    "    for col in full_sample.columns:\n",
    "        print(f\"- {col}\")\n",
    "    \n",
    "    print(\"\\nDetailed view of first product:\")\n",
    "    print(\"-\"*30)\n",
    "    first_product = full_sample.iloc[0]\n",
    "    \n",
    "    # Print basic info\n",
    "    print(\"\\nBasic Information:\")\n",
    "    print(f\"Product ID: {first_product['product_id']}\")\n",
    "    print(f\"Category (PT): {first_product['product_category_name']}\")\n",
    "    print(f\"Category (EN): {first_product['product_category_name_english']}\")\n",
    "    \n",
    "    # Print review information if available\n",
    "    if pd.notna(first_product.get('review_comment_message')):\n",
    "        print(\"\\nCustomer Reviews:\")\n",
    "        print(f\"Average Score: {first_product['review_score']:.2f}\")\n",
    "        print(\"Review Comments:\")\n",
    "        print(first_product['review_comment_message'])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze text content distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded olist_products_dataset.csv with 32951 rows\n",
      "Successfully loaded product_category_name_translation.csv with 71 rows\n",
      "Successfully loaded olist_order_items_dataset.csv with 112650 rows\n",
      "Successfully loaded olist_order_reviews_dataset.csv with 99224 rows\n",
      "Text Content Analysis:\n",
      "==================================================\n",
      "\n",
      "Products with reviews: 992 out of 1000 (99.2%)\n",
      "\n",
      "Category Language Distribution:\n",
      "- bed_bath_table: 118 products\n",
      "- furniture_decor: 108 products\n",
      "- sports_leisure: 81 products\n",
      "- housewares: 79 products\n",
      "- auto: 57 products\n",
      "\n",
      "Review Length Statistics:\n",
      "Average review length: 116 characters\n",
      "Max review length: 6445 characters\n",
      "Min review length: 0 characters\n"
     ]
    }
   ],
   "source": [
    "# Fourth cell - Analyze text content distribution\n",
    "try:\n",
    "    # Get a larger sample for analysis\n",
    "    analysis_sample = loader.get_full_product_data(n_samples=1000)\n",
    "    \n",
    "    print(\"Text Content Analysis:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Analyze review presence\n",
    "    reviews_present = analysis_sample['review_comment_message'].notna().sum()\n",
    "    print(f\"\\nProducts with reviews: {reviews_present} out of {len(analysis_sample)} ({reviews_present/len(analysis_sample)*100:.1f}%)\")\n",
    "    \n",
    "    # Analyze language distribution\n",
    "    print(\"\\nCategory Language Distribution:\")\n",
    "    category_counts = analysis_sample.groupby('product_category_name_english').size().sort_values(ascending=False).head()\n",
    "    for category, count in category_counts.items():\n",
    "        print(f\"- {category}: {count} products\")\n",
    "    \n",
    "    # Look at review lengths\n",
    "    analysis_sample['review_length'] = analysis_sample['review_comment_message'].str.len()\n",
    "    print(\"\\nReview Length Statistics:\")\n",
    "    print(f\"Average review length: {analysis_sample['review_length'].mean():.0f} characters\")\n",
    "    print(f\"Max review length: {analysis_sample['review_length'].max():.0f} characters\")\n",
    "    print(f\"Min review length: {analysis_sample['review_length'].min():.0f} characters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine review content quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded olist_products_dataset.csv with 32951 rows\n",
      "Successfully loaded product_category_name_translation.csv with 71 rows\n",
      "Successfully loaded olist_order_items_dataset.csv with 112650 rows\n",
      "Successfully loaded olist_order_reviews_dataset.csv with 99224 rows\n",
      "Detailed Text Analysis:\n",
      "==================================================\n",
      "\n",
      "Word Count Statistics:\n",
      "Average words per review: 20.2\n",
      "Median words per review: 4.5\n",
      "\n",
      "Reviews with Portuguese characters: 362 (36.2%)\n",
      "\n",
      "Sample Reviews by Length:\n",
      "\n",
      "Short Review Example:\n",
      "Voltarei a comprar!\n",
      "\n",
      "Medium Review Example:\n",
      "Adorei os livros e os cartazes, são excelentes para usarmos com nossos alunos\n",
      "\n",
      "Long Review Example:\n",
      "correta  | não usei o produto não mas parece bom | Eu comprei um relógio e me mandaram outro  | Melhor do Brasil | vou comprar de novo , gostei e é excelente os produtos obrigado! | na verdade o produ...\n"
     ]
    }
   ],
   "source": [
    "# Fifth cell - Examine review content quality\n",
    "try:\n",
    "    # Get another sample for detailed text analysis\n",
    "    text_sample = loader.get_full_product_data(n_samples=1000)\n",
    "    \n",
    "    # Add analysis of review text characteristics\n",
    "    text_sample['review_word_count'] = text_sample['review_comment_message'].str.split().str.len()\n",
    "    text_sample['has_portuguese_chars'] = text_sample['review_comment_message'].str.contains('[áéíóúãõâêîôûç]', regex=True)\n",
    "    \n",
    "    print(\"Detailed Text Analysis:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Word count distribution\n",
    "    print(\"\\nWord Count Statistics:\")\n",
    "    print(f\"Average words per review: {text_sample['review_word_count'].mean():.1f}\")\n",
    "    print(f\"Median words per review: {text_sample['review_word_count'].median():.1f}\")\n",
    "    \n",
    "    # Language detection\n",
    "    portuguese_reviews = text_sample['has_portuguese_chars'].sum()\n",
    "    print(f\"\\nReviews with Portuguese characters: {portuguese_reviews} ({portuguese_reviews/len(text_sample)*100:.1f}%)\")\n",
    "    \n",
    "    # Look at some examples of different lengths\n",
    "    print(\"\\nSample Reviews by Length:\")\n",
    "    print(\"\\nShort Review Example:\")\n",
    "    short_review = text_sample[text_sample['review_word_count'] < 5].iloc[0]['review_comment_message']\n",
    "    print(short_review)\n",
    "    \n",
    "    print(\"\\nMedium Review Example:\")\n",
    "    medium_review = text_sample[\n",
    "        (text_sample['review_word_count'] > 10) & \n",
    "        (text_sample['review_word_count'] < 20)\n",
    "    ].iloc[0]['review_comment_message']\n",
    "    print(medium_review)\n",
    "    \n",
    "    print(\"\\nLong Review Example:\")\n",
    "    long_review = text_sample[text_sample['review_word_count'] > 50].iloc[0]['review_comment_message']\n",
    "    print(long_review[:200] + \"...\" if len(long_review) > 200 else long_review)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language handling analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded olist_products_dataset.csv with 32951 rows\n",
      "Successfully loaded product_category_name_translation.csv with 71 rows\n",
      "Successfully loaded olist_order_items_dataset.csv with 112650 rows\n",
      "Successfully loaded olist_order_reviews_dataset.csv with 99224 rows\n",
      "Detailed Language Analysis:\n",
      "==================================================\n",
      "\n",
      "Total individual reviews analyzed: 1928\n",
      "Reviews with accents: 842 (43.7%)\n",
      "Reviews with Portuguese indicators: 1128 (58.5%)\n",
      "\n",
      "Length Distribution:\n",
      "Average length: 57.8 characters\n",
      "Shortest review: 0 characters\n",
      "Longest review: 203 characters\n"
     ]
    }
   ],
   "source": [
    "# Sixth cell - Language handling analysis\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "\n",
    "class TextAnalyzer:\n",
    "    \"\"\"Analyzes and processes bilingual review text.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Common Portuguese words that indicate the text is in Portuguese\n",
    "        self.portuguese_indicators = {\n",
    "            'produto', 'muito', 'bom', 'ótimo', 'excelente',\n",
    "            'recomendo', 'chegou', 'recebi', 'não', 'para'\n",
    "        }\n",
    "    \n",
    "    def analyze_review(self, text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Analyze a single review for language characteristics.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return {\n",
    "                'length': 0,\n",
    "                'has_accents': False,\n",
    "                'likely_portuguese': False,\n",
    "                'word_count': 0\n",
    "            }\n",
    "            \n",
    "        # Basic text cleanup\n",
    "        text = text.lower().strip()\n",
    "        \n",
    "        # Analyze characteristics\n",
    "        has_accents = bool(re.search('[áéíóúãõâêîôûç]', text))\n",
    "        words = text.split()\n",
    "        portuguese_words = sum(1 for word in words \n",
    "                             if word in self.portuguese_indicators)\n",
    "        \n",
    "        return {\n",
    "            'length': len(text),\n",
    "            'has_accents': has_accents,\n",
    "            'likely_portuguese': portuguese_words > 0,\n",
    "            'word_count': len(words)\n",
    "        }\n",
    "\n",
    "try:\n",
    "    # Create analyzer\n",
    "    analyzer = TextAnalyzer()\n",
    "    \n",
    "    # Get fresh sample\n",
    "    text_sample = loader.get_full_product_data(n_samples=1000)\n",
    "    \n",
    "    # Analyze reviews\n",
    "    analyses = []\n",
    "    for review in text_sample['review_comment_message'].dropna():\n",
    "        # Split combined reviews\n",
    "        individual_reviews = review.split(' | ')\n",
    "        for individual_review in individual_reviews:\n",
    "            analyses.append(analyzer.analyze_review(individual_review))\n",
    "    \n",
    "    # Compile statistics\n",
    "    total_reviews = len(analyses)\n",
    "    accented = sum(1 for a in analyses if a['has_accents'])\n",
    "    likely_portuguese = sum(1 for a in analyses if a['likely_portuguese'])\n",
    "    \n",
    "    print(\"Detailed Language Analysis:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nTotal individual reviews analyzed: {total_reviews}\")\n",
    "    print(f\"Reviews with accents: {accented} ({accented/total_reviews*100:.1f}%)\")\n",
    "    print(f\"Reviews with Portuguese indicators: {likely_portuguese} ({likely_portuguese/total_reviews*100:.1f}%)\")\n",
    "    \n",
    "    # Length distribution\n",
    "    lengths = [a['length'] for a in analyses]\n",
    "    print(f\"\\nLength Distribution:\")\n",
    "    print(f\"Average length: {sum(lengths)/len(lengths):.1f} characters\")\n",
    "    print(f\"Shortest review: {min(lengths)} characters\")\n",
    "    print(f\"Longest review: {max(lengths)} characters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Processing Result:\n",
      "==================================================\n",
      "Cleaned text: correta nao usei produto nao mas parece bom comprei relogio mandaram outro melhor brasil vou comprar...\n",
      "\n",
      "Multilingual Processing Result:\n",
      "==================================================\n",
      "Original text: correta  | não usei o produto não mas parece bom | Eu comprei um relógio e me mandaram outro  | Melh...\n",
      "Cleaned text: correta usei produto mas parece bom comprei rel gio mandaram outro melhor brasil vou comprar novo go...\n",
      "Detected language: pt\n",
      "Has accents: True\n",
      "Word count: 143\n"
     ]
    }
   ],
   "source": [
    "# 7th cell - Test the enhanced preprocessing\n",
    "from vectorshop.data.preprocessing import TextPreprocessor, MultilingualTextPreprocessor, ProcessedText\n",
    "\n",
    "# Test the enhanced preprocessing\n",
    "try:\n",
    "    # Create both preprocessors\n",
    "    basic_processor = TextPreprocessor(remove_accents=True)\n",
    "    multilingual_processor = MultilingualTextPreprocessor(remove_accents=False)\n",
    "    \n",
    "    # Get a sample product with review\n",
    "    sample = text_sample.iloc[0]\n",
    "    \n",
    "    # Process with both processors\n",
    "    print(\"Basic Processing Result:\")\n",
    "    print(\"=\" * 50)\n",
    "    basic_result = basic_processor.clean_text(sample['review_comment_message'])\n",
    "    print(f\"Cleaned text: {basic_result[:100]}...\")\n",
    "    \n",
    "    print(\"\\nMultilingual Processing Result:\")\n",
    "    print(\"=\" * 50)\n",
    "    ml_result = multilingual_processor.process_with_metadata(sample['review_comment_message'])\n",
    "    print(f\"Original text: {ml_result.original[:100]}...\")\n",
    "    print(f\"Cleaned text: {ml_result.cleaned[:100]}...\")\n",
    "    print(f\"Detected language: {ml_result.language}\")\n",
    "    print(f\"Has accents: {ml_result.has_accents}\")\n",
    "    print(f\"Word count: {ml_result.word_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze a batch of reviews with our multilingual processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Language Analysis:\n",
      "==================================================\n",
      "Total reviews analyzed: 50\n",
      "Portuguese reviews: 20 (40.0%)\n",
      "English reviews: 30 (60.0%)\n",
      "Reviews with accents: 20 (40.0%)\n",
      "Average words per review: 33.4\n",
      "\n",
      "Portuguese Review Example:\n",
      "------------------------------\n",
      "Original: correta  | não usei o produto não mas parece bom | Eu comprei um relógio e me mandaram outro  | Melh...\n",
      "Cleaned: correta usei produto mas parece bom comprei rel gio mandaram outro melhor brasil vou comprar novo go...\n",
      "\n",
      "English Review Example:\n",
      "------------------------------\n",
      "Original: Voltarei a comprar!...\n",
      "Cleaned: voltarei comprar...\n"
     ]
    }
   ],
   "source": [
    "# 8th cell - Analyze a batch of reviews with our multilingual processor\n",
    "try:\n",
    "    multilingual_processor = MultilingualTextPreprocessor(remove_accents=False)\n",
    "    \n",
    "    # Get a sample of reviews\n",
    "    reviews_sample = text_sample['review_comment_message'].head(50)\n",
    "    \n",
    "    # Process each review and collect statistics\n",
    "    stats = {\n",
    "        'pt_count': 0,\n",
    "        'en_count': 0,\n",
    "        'accented_count': 0,\n",
    "        'avg_word_count': 0,\n",
    "        'samples': {\n",
    "            'pt_sample': None,\n",
    "            'en_sample': None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    total_words = 0\n",
    "    for review in reviews_sample:\n",
    "        if isinstance(review, str):\n",
    "            result = multilingual_processor.process_with_metadata(review)\n",
    "            \n",
    "            # Update statistics\n",
    "            if result.language == 'pt':\n",
    "                stats['pt_count'] += 1\n",
    "                if not stats['samples']['pt_sample']:\n",
    "                    stats['samples']['pt_sample'] = result\n",
    "            else:\n",
    "                stats['en_count'] += 1\n",
    "                if not stats['samples']['en_sample']:\n",
    "                    stats['samples']['en_sample'] = result\n",
    "                    \n",
    "            if result.has_accents:\n",
    "                stats['accented_count'] += 1\n",
    "                \n",
    "            total_words += result.word_count\n",
    "    \n",
    "    stats['avg_word_count'] = total_words / len(reviews_sample) if len(reviews_sample) > 0 else 0\n",
    "    \n",
    "    # Print analysis results\n",
    "    print(\"Review Language Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total reviews analyzed: {len(reviews_sample)}\")\n",
    "    print(f\"Portuguese reviews: {stats['pt_count']} ({stats['pt_count']/len(reviews_sample)*100:.1f}%)\")\n",
    "    print(f\"English reviews: {stats['en_count']} ({stats['en_count']/len(reviews_sample)*100:.1f}%)\")\n",
    "    print(f\"Reviews with accents: {stats['accented_count']} ({stats['accented_count']/len(reviews_sample)*100:.1f}%)\")\n",
    "    print(f\"Average words per review: {stats['avg_word_count']:.1f}\")\n",
    "    \n",
    "    # Show examples\n",
    "    if stats['samples']['pt_sample']:\n",
    "        print(\"\\nPortuguese Review Example:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Original: {stats['samples']['pt_sample'].original[:100]}...\")\n",
    "        print(f\"Cleaned: {stats['samples']['pt_sample'].cleaned[:100]}...\")\n",
    "    \n",
    "    if stats['samples']['en_sample']:\n",
    "        print(\"\\nEnglish Review Example:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Original: {stats['samples']['en_sample'].original[:100]}...\")\n",
    "        print(f\"Cleaned: {stats['samples']['en_sample'].cleaned[:100]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
